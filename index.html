<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Robotics Portfolio</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Poppins:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap" rel="stylesheet">
</head>
<body>
    <header>
        <h1 id="header">Kane Li Robotics Portfolio</h1>
    </header>
    <main>
        <section id="about-me">
            <h2>About Me</h2>
            <p>
                Hello, my name is Kane Li! Currently I am an Autonomy lead for Triton Robotics, a team that participates in 3 vs 3 robot battles.
                According to the rules, one of the 3 robots must be fully autonomous, and that is where I come in. My subteam has been
                working to develop an autonomous system from the ground up, shown through a couple of the projects below. I hope
                to continue to develop my skills in order to learn best practices and new skills that will only only improve my own
                abilities, but also enable me to take those skills back to my team and help them grow as well. My current interests include robotics sensing, 
                motion-planning, and computer vision.
            </p>
        </section>
        <section id="projects">
            <div class="project">
                <h2>Serial Communications</h2>
                <video controls muted>
                    <source src="./resources/serial_comms.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <p class="project-description">
                    This project was my first real task in Triton Robotics, where I helped to establish communications so our code that runs on
                    an NVIDIA Jetson can communicate with the Embedded team's STM32 to make the robot move. After talking with a member from Embedded
                    to decide upon a protocol, we used UART to send data between each other. Once communication was established, I then created a ROS2
                    service that allows other nodes to both read requests and write requests to acess the serial data. Currently, we are working on 
                    improving the real-time constraints of the system, addressing data misalignment, inconsistent latencies, and data corruption.
                </p>
                <ul>
                    <li>The video above is an old example of receiving current robot position and velocity from the Embedded side</li>
                    <li>Data was sent by writing bytes to the file "/ttySH1" (or "/USB0" if on a laptop), and verified using an LRC Checksum</li>
                    <li>Our BAUD rate is around 115200 bits per second. The wiring is from the Jetson ttl pinout to JST pin connection</li>
                </ul>
            </div>
            <div class="project">
                <h2>Simulations</h2>
                <video controls>
                    <source src="./resources/sim_video.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <p class="project-description">
                    Once I became team lead, I expanded the size of the team, which requires more ways for people to run code on the "robot".
                    Therefore, I wrote this a pybullet simulation that loads a URDF model that interfaces with ROS. Users are able to control 
                    the robot movement using the arrow keys, and the camera view is calculated using projection and view matrices. There also is
                    a ROS2 service that handles moving the pitch and yaw of the turret, mimicking serial communication with our real robot.
                </p>
                <ul>
                    <li>The video demonstrates a recruit's work on testing SLAM algorithms with a simualted LIDAR. We simulated a LIDAR by shooting ray batches and checking for collisions.</li>
                    <li>Camera view was calculated using projection and view matrices, based on the forward direction from the turret link</li>
                    <li>The ROS2 node keeps track of desired pitch and yaw, and updates pybullet's joints to move in that direction</li>
                </ul>
            </div>
            <div class="project">
                <h2>Image Processing</h2>
                <video controls>
                    <source src="./resources/image_processing.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <p class="project-description">
                    Currently, the robot uses a yolov5 model to conduct image segmentation on camera imagery to find bounding boxes for armor plates.
                    We are also working to extend this functionality to determine the color, type, and distance that the armor plate is from the robot.
                    To do so, we utilize OpenCV, where in the video above we are drawing the bounding boxes, coloring the bounding box according to the
                    armor plate color, and drawing where we determine the lightbars to be (for solve PnP). A major point of interest for us is to reduce
                    the latency of communication between our camera, detector, and image processor using no-copy message transport since our current aiming
                    system is not as responsive as we would like.
                </p>
                <ul>
                    <li>The video demonstrates the program drawing matching color bounding boxes with white lines for where it detects lightbars</li>
                    <li>There was a little delay to the video since the image is only updated whenever the robot sees a detection</li>
                    <li>Our image segmentation model runs on the GPU using CUDA, and is trained using a team labeled custom dataset</li>
                </ul>
            </div>
        </section>
    </main>
</body>
</html>